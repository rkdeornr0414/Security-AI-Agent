# Security-AI-Agent

# Diagram


<img width="563" height="389" alt="image" src="https://github.com/user-attachments/assets/b7294d14-49fb-4abe-b97c-b0b3178ab124" />

# Key Principle
No single layer is sufficient. Prompt injection is an unsolved problem in AI security â€” the LLM can always potentially be tricked. That's why the architectural layers (secrets the agent can never access, output filtering, network restrictions) are more important than trying to make the prompt "un-injectable."


